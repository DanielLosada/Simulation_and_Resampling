## Daniel Losada and Sergio Quintanilla

# Exercise 1

```{r}
simulate_T <- function(n) {
  U <- runif(n)  # Generate n uniform random numbers
  T <- 2 / (1 - U)^(1/3)  # Apply inverse transform
  return(T)
}

# Example usage
set.seed(123)
samples <- simulate_T(10000)

# Plot histogram
hist(samples, probability = TRUE, col = "lightblue", breaks = 50, main = "Simulated Samples from F_T(t)", xlab = "T")

# Overlay theoretical density (Derivative of CDF)
curve(6 * (2^3) / x^4, from = 2, to = max(samples), add = TRUE, col = "red", lwd = 2)

```

```{r}
library(Rcpp)

cppFunction('
NumericVector simulate_T_rcpp(int n) {
  NumericVector U = runif(n);  // Generate uniform samples
  NumericVector T = 2 / pow(1 - U, 1.0 / 3.0);  // Apply inverse transform
  return T;
}
')

# Example usage
set.seed(123)
samples_rcpp <- simulate_T_rcpp(10000)

# Verify by histogram
hist(samples_rcpp, probability = TRUE, col = "lightblue", breaks = 50, main = "Simulated Samples from F_T(t) - Rcpp", xlab = "T")
curve(6 * (2^3) / x^4, from = 2, to = max(samples_rcpp), add = TRUE, col = "red", lwd = 2)

```

# Exercise 2

```{r}
library(ggplot2)

### Part (a): Determine M such that f_S(t) â‰¤ M f_T(t) ###
f_S <- function(t) { 8 / t^5 }  # PDF of S ~ Pareto(2,4)
f_T <- function(t) { 6 / t^4 }  # PDF of T ~ Pareto(2,3)

# Compute M
M <- max((f_S(2) / f_T(2)))  # Maximized at t = 2
cat("(a) M =", M, "\n")  # Should output M = 2/3

### Part (b): Rejection Sampling Algorithm ###
# Function to sample from Pareto(2,3) using inverse transform sampling
rpareto_T <- function(n) {
  U <- runif(n)
  return(2 / (U^(1/3)))  # Inverse CDF
}

# Acceptance-rejection sampling for S ~ Pareto(2,4)
rpareto_S <- function(n) {
  samples <- numeric(n)
  count <- 0
  while (count < n) {
    T <- rpareto_T(1)  # Sample T ~ Pareto(2,3)
    U <- runif(1)
    if (U <= 2 / T) {  # Acceptance condition
      count <- count + 1
      samples[count] <- T
    }
  }
  return(samples)
}

### Part (c): Monte Carlo Estimation of E[S] and Confidence Interval ###
set.seed(123)
MC <- 10000
samples_S <- rpareto_S(MC)

# Compute expected value
E_S <- mean(samples_S)
cat("(c) Estimated E[S]:", E_S, "\n")

# Compute 99% confidence interval
alpha <- 0.01
se <- sd(samples_S) / sqrt(MC)
CI <- c(E_S - qnorm(1 - alpha/2) * se, E_S + qnorm(1 - alpha/2) * se)
cat("(c) 99% Confidence Interval: [", CI[1], ",", CI[2], "]\n")

# Plot histogram with theoretical density
ggplot(data.frame(S = samples_S), aes(x = S)) +
  geom_histogram(aes(y = ..density..), bins = 50, fill = "lightblue", color = "black", alpha = 0.7) +
  stat_function(fun = f_S, color = "red", lwd = 1.2) +
  labs(title = "Simulated Samples from Pareto(2,4)", x = "S", y = "Density") +
  theme_minimal()

```

# Excercise 3

First, let's observe what we expect to see using the truncnorm library's random sampling distribution.

```{r}
library(truncnorm)
plot(density(rtruncnorm(1000,a=-1,b=1,mean=0,sd=1))) 
```

Now let's test the different sampling methods.

## a)
```{r, warning=FALSE}
AR_norm = function(n, off=F){
  data = AR.Sim ( n = n ,
    f_X = function (y){ dtruncnorm(y,-1,1,0,1)} ,
    Y.dist = "norm" , Y.dist.par = c (0,1) ,
    Rej.Num = TRUE , Rej.Rate = TRUE , Acc.Rate = TRUE )
  if (off == T) {dev.off()} #to block out plots in efficiency test
  plot(density(data))
  if (off == T) {dev.off()}

}
AR_norm(500)
```

## b)

```{r, warning=FALSE}
AR_unif = function(n,off=F){
  data = AR.Sim ( n = n ,
    f_X = function (y){ dtruncnorm(y,-1,1,0,1)} ,
    Y.dist = "unif" , Y.dist.par = c (-1,1) ,
    Rej.Num = TRUE , Rej.Rate = TRUE , Acc.Rate = TRUE)
  if (off == T) {dev.off()} #to block out plots in efficiency test
  plot(density(data))
  if (off == T) {dev.off()}

}

AR_unif(500)
```

Overall, a and b look quite similar, although not exactly, but both estimate a truncated distribution quite well. The uniform one appears quite similar to a common normal distribution, indicating it might be underestimating the variance around the mean. However, the normal version appears to have more skew, which would make sense given it has another layer of bias, depending on the candidate normal distribution's behavior.

## c)
```{r}

I_norm = function(n, off=F){
  #set our parameters for the truncated distribution
  mean <- 0
  sd <- 1
  a <- -1
  b <- 1
  u <- runif(n)
  
  #apply the inverse CDF to transform the uniform rv to a truncated normal
  sim_inv <- qtruncnorm(u, mean = mean, sd = sd, a = a, b = b)
  if (off == F) {plot(density(sample_truncated_normal))} #to block out plots in efficiency test
}
I_norm(500)

```

This one appears mostly like a regular normal distribution with some irregularities, but is not as close as the prior two sampling methods.

```{r, warning=FALSE}
library(microbenchmark)
n=500
microbenchmark( #use microbenchmark to compare efficiencies
  AR_norm = AR_norm(n, off=T),
  AR_unif = AR_unif(n, off=T),
  I_norm = I_norm(n, off=T),
  times = 30
)

```

We can see that the inverse transform technique was by far the fastest, exponentially faster than either Accept-Reject algorithm. Within AR algorithms, the uniform candidate density took half the time the normal one did. That being said, looking at the graphs above, the distributions appear to be more accurately estimated by the normal AR algorithm, which makes sense given we expect a tradeoff between accuracy and speed.

