```{r}
# Load necessary libraries
library(datasets)
library(stats)

data("iris")
```

# Exercise 1

## a)
We can't reject the H0. So we don't have reasons to believe that the data is not normally distributed.

```{r}
setosa_sepal_length <- iris$Sepal.Length[iris$Species == "setosa"]
print(setosa_sepal_length)

shapiro_test <- shapiro.test(setosa_sepal_length)
print(shapiro_test)
```

## b)

```{r}
sample_mean <- mean(setosa_sepal_length)
print(sample_mean)
sample_sd <- sd(setosa_sepal_length)
print(sample_sd)
ks_test <- ks.test(setosa_sepal_length, "pnorm", sample_mean, sample_sd)
D_statistic <- ks_test$statistic
print(D_statistic)
```

## c)
```{r}
MC <- 1000
n <- 50
D_simulated <- numeric(MC)

set.seed(42)  
for (i in 1:MC) {
  sample_data <- rnorm(n, mean = mean(setosa_sepal_length), sd = sd(setosa_sepal_length))
  D_simulated[i] <- ks.test(sample_data, "pnorm", mean(sample_data), sd(sample_data))$statistic
}
D_simulated
```

## d)
We aproximate the p-value by checking how many of the simulated D statistics with unespecified parameters are greater that the one computed directly from the sample with specified parameters.
We can't reject the H0. So we don't have reasons to believe that the data is not normally distributed.
```{r}
p_value_approx <- mean(D_simulated > D_statistic)
print(p_value_approx)
```

# Exercise 2

```{r}
set.seed(42)

claim_prob_high <- 0.02
claim_prob_medium <- 0.01
claim_prob_low <- 0.0025

prob_high_risk <- 0.1
prob_medium_risk <- 0.2
prob_low_risk <- 0.7

MC <- 10000  # Number Monte Carlo claims
claims_high_risk <- 0 # Number of high risk claims
count_claims <- 0 # Total claims counter

while (count_claims < MC) {
  # Select random client
  client_type <- sample(c("high", "medium", "low"), size = 1, prob = c(prob_high_risk, prob_medium_risk, prob_low_risk), replace = TRUE)

  # Determine if the selected client makes a claim based on the probability. We just pick a random value between 0 and 1 (runif) and check if its smaller than the prob defined. 
  if (client_type == "high" && runif(1) < claim_prob_high) {
    claims_high_risk <- claims_high_risk + 1
    count_claims <- count_claims + 1
  } else if (client_type == "medium" && runif(1) < claim_prob_medium) {
    count_claims <- count_claims + 1
  } else if (client_type == "low" && runif(1) < claim_prob_low) {
    count_claims <- count_claims + 1
  }
}

# Calculate the ratio of claims of high risk out of the 10000 claims. 
#That will be an aproximation of the probability of a claim coming from a high risk client.
prob_estimate <- claims_high_risk / MC

# We compute the confidence interval using the formula p +- z_value*sqrt(p(1-p)/n). Where p=prob of being high risk. n = n clients.
# We can do this because we have big enough n to apply the CLT and use the normal aproximation to the binomial. (Is high risk vs Is not high risk)
alpha <- 0.01
z_value <- qnorm(1-alpha/2)  # 99% confidence level
stderr <- sqrt(prob_estimate * (1 - prob_estimate) / MC)
ci_lower <- prob_estimate - z_value * stderr
ci_upper <- prob_estimate + z_value * stderr

cat("Estimated probability:", prob_estimate, "\n")
cat("99% Confidence Interval: [", ci_lower, ",", ci_upper, "]\n")
```

# Exercise 3

```{r}

set.seed(234)

MC <- 1000

f <- function(x) {
  return(exp(exp(x)))
}

gen_samples <- function(n) {
  u <- runif(n, 0, 1) 
  samples <- f(u)      
  return(samples)
}

sample <- gen_samples(MC)

#Based on the LLN, the integral of this sample can be approximated by the sample mean, so we can just calculate it from the samples we have and approximate that integral.

get_parameters <- function(samp) {
  n <- length(samp)
  s_mean <- mean(samp)  
  s_var <- var(samp)    
  
  CI <- c(
    s_mean - (qnorm(0.975) * (sqrt(s_var) / sqrt(n))),
    s_mean + (qnorm(0.975) * (sqrt(s_var) / sqrt(n)))
  )
  
  return(list(s_mean = s_mean, CI = CI))
}

result <- get_parameters(sample)
print(result)

# To get a 99%CI with a width of 0.5, we can just try different sample sizes.

size_test <- function(n) {
  sample <- gen_samples(n)
  integral_CI <- get_parameters(sample)
  width <- integral_CI$CI[2] - integral_CI$CI[1]
  return(width)
}

width <- size_test(700)
print(width)

# From this, we can determine that 700 samples would get us a 99% CI with a width of 0.5

```

# Exercise 4

We can approximate the integrals using the previous monte carlo approximation

```{r}
library(pracma)

gen_samples <- function(n, func, a, b) {
  u <- runif(n, 0, 1) 
  u <- a + ((b - a) * u) 
  samples <- func(u) 
  return(samples)
}

mc_integral <- function(a, b, n, func) {
  sample <- gen_samples(n, func, a, b)
  integral <- (b - a) * mean(sample)
  return(integral)
}

# 1
f1 <- function(x) {
  return(exp(x + x^2))
}

result_quad <- integral(f1, -2, 2)
result_mc <- mc_integral(-2, 2, 10000, f1)
print(c(result_quad, result_mc))
```

In this first example, we get pretty close to the real value using monte carlo approximations.

```{r}
gen_samples_2d <- function(n, func, a, b) {
  u <- runif(n, 0, 1) 
  v <- runif(n, 0, 1) 
  u <- a + ((b - a) * u)
  v <- a + ((b - a) * v)
  samples <- func(u, v) 
  return(samples)
}

mc_integral_2d <- function(a, b, n, func) {
  sample <- gen_samples_2d(n, func, a, b)
  integral <- (b - a)^2 * mean(sample)
  return(integral)
}

#2
f2 <- function(x, y) {
  return(exp((x + y)^2))
}

result_quad2d <- integral2(f2, 0, 1, 0, 1)$Q
result_mc2d <- mc_integral_2d(0, 1, 1000, f2)
print(c(result_quad2d, result_mc2d))
```

Here, we also get quite close to the actual value, although with a good bit of variation.

```{r}
gen_samples_variable_limits <- function(n, func) {
  u <- runif(n, 0, 100000) 
  v <- runif(n, 0, u) 
  samples <- func(v, u)
  return(samples)
}

mc_integral_variable_limits <- function(n, func) {
  sample <- gen_samples_variable_limits(n, func)
  integral <- mean(sample)  # Estimate the integral
  return(integral)
}

#3
f3 <- function(y, x) {
  return(exp(-(x + y)))
}

upper_limit_y <- function(x) {
  return(x)
}
result_quad2d_variable <- integral2(f3, 0, 100000, 0, upper_limit_y)$Q
result_mc2d_variable <- mc_integral_variable_limits(1000, f3)
print(c(result_quad2d_variable, result_mc2d_variable))
```

Here, we reach the described value only occasionally and only when using large numbers rather than infinity, since we can't take infinite samples. In this case, monte carlo integration suffers from the limitations of random sampling against asymptotic limits.

